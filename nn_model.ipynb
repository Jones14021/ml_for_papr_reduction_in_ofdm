{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load training data from .dat files\n",
    "raw_data_imaginary_x.dat --> train_imag\n",
    "raw_data_real_x.dat      --> train_real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if IS_TRAINING == True, a new model will be compiled \n",
    "# and trained from the dataset\n",
    "#\n",
    "# if IS_TRAINING == False, an existing model will be \n",
    "# loaded and tested against the dataset\n",
    "IS_TRAINING = True\n",
    "\n",
    "# trainingset to be trained or loaded\n",
    "trainingset_number = 5\n",
    "\n",
    "# training params\n",
    "neuron_number = 8\n",
    "activation_function = 'tanh'\n",
    "iterations = 20000\n",
    "learning_rate= 1e-4\n",
    "\n",
    "checkpoint_path = \"./training_\" + str(trainingset_number) + \"/cp_test.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_raw_data_real = \"raw_data_real_\" + str(trainingset_number) + \".dat\"\n",
    "filename_raw_data_imag = \"raw_data_imaginary_\" + str(trainingset_number) + \".dat\"\n",
    "\n",
    "filename_dataset_csv = \"training_data_\" + str(trainingset_number) + \".csv\"\n",
    "\n",
    "# my_file = open(filename_raw_data_real, \"r\")\n",
    "# lines_real = my_file.readlines()\n",
    "# my_file.close()\n",
    "\n",
    "# my_file = open(filename_raw_data_imag, \"r\")\n",
    "# lines_imag = my_file.readlines()\n",
    "# my_file.close()\n",
    "\n",
    "# train_real = []\n",
    "# train_imag = []\n",
    "\n",
    "# for line in lines_real:\n",
    "#     train_real = [train_real ,float(line)]\n",
    "\n",
    "# for line in lines_imag:\n",
    "#     train_imag = [train_imag ,float(line)]\n",
    "\n",
    "# dataset_real = tf.data.TextLineDataset(filename_raw_data_real)\n",
    "# dataset_imag = tf.data.TextLineDataset(filename_raw_data_imag)\n",
    "\n",
    "# # convert strings to float\n",
    "# def parse_fnc(line):\n",
    "#     string_vals = tf.strings.split([line]).values\n",
    "#     return tf.strings.to_number(string_vals, tf.float32)\n",
    "\n",
    "# dataset_real = dataset_real.map(map_func=parse_fnc)\n",
    "# dataset_imag = dataset_imag.map(map_func=parse_fnc)\n",
    "\n",
    "# # merge datasets\n",
    "# dataset = dataset_real.zip(dataset_imag)\n",
    "\n",
    "dataset = tf.data.experimental.CsvDataset(\n",
    "  filename_dataset_csv,\n",
    "  [tf.float32,  # Required field, use dtype or empty tensor\n",
    "   tf.float32,\n",
    "   tf.float32,\n",
    "   tf.float32\n",
    "  ],\n",
    "  header=True,\n",
    "  select_cols=[0,1,2,3]  \n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<CsvDatasetV2 element_spec=(TensorSpec(shape=(), dtype=tf.float32, name=None), TensorSpec(shape=(), dtype=tf.float32, name=None), TensorSpec(shape=(), dtype=tf.float32, name=None), TensorSpec(shape=(), dtype=tf.float32, name=None))>\n",
      "(0.09036217, -0.2613757, 0.22294177, 0.1604512)\n",
      "(0.022515569, -0.1132093, 0.16743372, 0.06361104)\n",
      "(0.114548385, -0.100870565, 0.12522025, -0.011140993)\n",
      "(0.049135324, -0.014699027, 0.098613776, -0.06175121)\n",
      "(-0.13331392, -0.12705529, 0.087487355, -0.089262664)\n",
      "(0.1544656, -0.3077073, 0.089055814, -0.09597649)\n",
      "(0.11328178, -0.026650935, 0.09796924, -0.08606317)\n",
      "(0.27299157, -0.015948214, 0.10823633, -0.06591184)\n",
      "(0.15798786, 0.18548791, 0.11556848, -0.042117316)\n",
      "(0.11445405, 0.01783486, 0.11829203, -0.018676687)\n",
      "(0.06248361, -0.12510371, 0.116487764, 0.0037648869)\n"
     ]
    }
   ],
   "source": [
    "print(dataset)\n",
    "i = 0\n",
    "\n",
    "for element in dataset.as_numpy_iterator():\n",
    "  print(element)\n",
    "  i = i + 1\n",
    "  if i > 10:\n",
    "      break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IS_TRAINING:\n",
    "    FC_layer = tf.keras.layers.Dense(units=neuron_number, activation=activation_function) # dense layer == fully connected layer\n",
    "\n",
    "    input_layer = tf.keras.layers.Dense(units=2, input_dim=2, activation=activation_function)\n",
    "    output_layer = tf.keras.layers.Dense(units=2, activation=activation_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model\n",
    "if IS_TRAINING:\n",
    "    model = tf.keras.Sequential([input_layer, FC_layer, output_layer])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function\n",
    "# as described in article: MSE mean squared error\n",
    "# batches have the format:\n",
    "# [batch_real, batch_imag]\n",
    "def my_loss_fcn(ground_truth_batch, predicted_batch):\n",
    "    result = tf.square(predicted_batch[:,1] - ground_truth_batch[:,1])\n",
    "    result = result + tf.square(predicted_batch[:,2] - ground_truth_batch[:,2])\n",
    "    N = len(ground_truth_batch)\n",
    "    result = tf.reduce_sum(result,1)\n",
    "    return result  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the keras model\n",
    "# loss=tf.keras.losses.MeanSquaredError()\n",
    "if IS_TRAINING:\n",
    "    model.compile(loss=my_loss_fcn, \n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate), \n",
    "    metrics=['accuracy',\n",
    "    tf.keras.metrics.RootMeanSquaredError()])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model from file if not training\n",
    "if not IS_TRAINING:\n",
    "    model = tf.keras.models.load_model(\"./training_\" + str(trainingset_number) + \"/saved_trained_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_13 (Dense)            (None, 2)                 6         \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 8)                 24        \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 2)                 18        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 48\n",
      "Trainable params: 48\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Display the model's architecture\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a callback that saves the model's weights\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=1)\n",
    "\n",
    "# This may generate warnings related to saving the state of the optimizer.\n",
    "# These warnings (and similar warnings throughout this notebook)\n",
    "# are in place to discourage outdated usage, and can be ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20000\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"c:\\Users\\jonas\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1021, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\jonas\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1010, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\jonas\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1000, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Users\\jonas\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 856, in train_step\n        x, y, sample_weight = data_adapter.unpack_x_y_sample_weight(data)\n    File \"c:\\Users\\jonas\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\data_adapter.py\", line 1577, in unpack_x_y_sample_weight\n        raise ValueError(error_msg)\n\n    ValueError: Data is expected to be in format `x`, `(x,)`, `(x, y)`, or `(x, y, sample_weight)`, found: (<tf.Tensor 'IteratorGetNext:0' shape=() dtype=float32>, <tf.Tensor 'IteratorGetNext:1' shape=() dtype=float32>, <tf.Tensor 'IteratorGetNext:2' shape=() dtype=float32>, <tf.Tensor 'IteratorGetNext:3' shape=() dtype=float32>)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\jonas\\OneDrive\\Dokumente\\Airbus_Helicopters\\DHBW\\TEN19\\Studienarbeit\\source_code_omar_project\\nn_model.ipynb Cell 13'\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jonas/OneDrive/Dokumente/Airbus_Helicopters/DHBW/TEN19/Studienarbeit/source_code_omar_project/nn_model.ipynb#ch0000008?line=0'>1</a>\u001b[0m \u001b[39m# fit the keras model on the dataset\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jonas/OneDrive/Dokumente/Airbus_Helicopters/DHBW/TEN19/Studienarbeit/source_code_omar_project/nn_model.ipynb#ch0000008?line=1'>2</a>\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jonas/OneDrive/Dokumente/Airbus_Helicopters/DHBW/TEN19/Studienarbeit/source_code_omar_project/nn_model.ipynb#ch0000008?line=2'>3</a>\u001b[0m \u001b[39m# train the model by slicing the data into \"batches\" of size batch_size,\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jonas/OneDrive/Dokumente/Airbus_Helicopters/DHBW/TEN19/Studienarbeit/source_code_omar_project/nn_model.ipynb#ch0000008?line=3'>4</a>\u001b[0m \u001b[39m# and repeatedly iterating over the entire dataset \u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jonas/OneDrive/Dokumente/Airbus_Helicopters/DHBW/TEN19/Studienarbeit/source_code_omar_project/nn_model.ipynb#ch0000008?line=4'>5</a>\u001b[0m \u001b[39m# for a given number of epochs\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jonas/OneDrive/Dokumente/Airbus_Helicopters/DHBW/TEN19/Studienarbeit/source_code_omar_project/nn_model.ipynb#ch0000008?line=5'>6</a>\u001b[0m \u001b[39mif\u001b[39;00m IS_TRAINING:\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/jonas/OneDrive/Dokumente/Airbus_Helicopters/DHBW/TEN19/Studienarbeit/source_code_omar_project/nn_model.ipynb#ch0000008?line=6'>7</a>\u001b[0m     history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(dataset, \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jonas/OneDrive/Dokumente/Airbus_Helicopters/DHBW/TEN19/Studienarbeit/source_code_omar_project/nn_model.ipynb#ch0000008?line=7'>8</a>\u001b[0m     epochs\u001b[39m=\u001b[39;49miterations)\n",
      "File \u001b[1;32mc:\\Users\\jonas\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/jonas/AppData/Local/Programs/Python/Python310/lib/site-packages/keras/utils/traceback_utils.py?line=64'>65</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     <a href='file:///c%3A/Users/jonas/AppData/Local/Programs/Python/Python310/lib/site-packages/keras/utils/traceback_utils.py?line=65'>66</a>\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m---> <a href='file:///c%3A/Users/jonas/AppData/Local/Programs/Python/Python310/lib/site-packages/keras/utils/traceback_utils.py?line=66'>67</a>\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     <a href='file:///c%3A/Users/jonas/AppData/Local/Programs/Python/Python310/lib/site-packages/keras/utils/traceback_utils.py?line=67'>68</a>\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     <a href='file:///c%3A/Users/jonas/AppData/Local/Programs/Python/Python310/lib/site-packages/keras/utils/traceback_utils.py?line=68'>69</a>\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\jonas\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:1147\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func.<locals>.autograph_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/jonas/AppData/Local/Programs/Python/Python310/lib/site-packages/tensorflow/python/framework/func_graph.py?line=1144'>1145</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint:disable=broad-except\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/jonas/AppData/Local/Programs/Python/Python310/lib/site-packages/tensorflow/python/framework/func_graph.py?line=1145'>1146</a>\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m\"\u001b[39m\u001b[39mag_error_metadata\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m-> <a href='file:///c%3A/Users/jonas/AppData/Local/Programs/Python/Python310/lib/site-packages/tensorflow/python/framework/func_graph.py?line=1146'>1147</a>\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mag_error_metadata\u001b[39m.\u001b[39mto_exception(e)\n\u001b[0;32m   <a href='file:///c%3A/Users/jonas/AppData/Local/Programs/Python/Python310/lib/site-packages/tensorflow/python/framework/func_graph.py?line=1147'>1148</a>\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   <a href='file:///c%3A/Users/jonas/AppData/Local/Programs/Python/Python310/lib/site-packages/tensorflow/python/framework/func_graph.py?line=1148'>1149</a>\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"c:\\Users\\jonas\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1021, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\jonas\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1010, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\jonas\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1000, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Users\\jonas\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 856, in train_step\n        x, y, sample_weight = data_adapter.unpack_x_y_sample_weight(data)\n    File \"c:\\Users\\jonas\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\data_adapter.py\", line 1577, in unpack_x_y_sample_weight\n        raise ValueError(error_msg)\n\n    ValueError: Data is expected to be in format `x`, `(x,)`, `(x, y)`, or `(x, y, sample_weight)`, found: (<tf.Tensor 'IteratorGetNext:0' shape=() dtype=float32>, <tf.Tensor 'IteratorGetNext:1' shape=() dtype=float32>, <tf.Tensor 'IteratorGetNext:2' shape=() dtype=float32>, <tf.Tensor 'IteratorGetNext:3' shape=() dtype=float32>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# fit the keras model on the dataset\n",
    "#\n",
    "# train the model by slicing the data into \"batches\" of size batch_size,\n",
    "# and repeatedly iterating over the entire dataset \n",
    "# for a given number of epochs\n",
    "if IS_TRAINING:\n",
    "    history = model.fit(dataset, \n",
    "    epochs=iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the dataset against the loaded model\n",
    "if not IS_TRAINING:\n",
    "    model.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model in the correct directory\n",
    "if IS_TRAINING:\n",
    "    model.save(\"./training_\" + str(trainingset_number) + \"/saved_trained_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The returned history object holds a record of the loss values \n",
    "# and metric values during training\n",
    "print(history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training progress\n",
    "# diagram number 2 paper page 4\n",
    "\n",
    "# cost function mean square error"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "683e9bbf599fde3b00e37a0db68ad40a268db525b46af3924c3427b16ddb8792"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
