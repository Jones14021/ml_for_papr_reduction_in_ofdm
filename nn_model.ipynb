{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load training data from .dat files\n",
    "raw_data_imaginary_x.dat --> train_imag\n",
    "raw_data_real_x.dat      --> train_real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if IS_TRAINING == True, a new model will be compiled \n",
    "# and trained from the dataset\n",
    "#\n",
    "# if IS_TRAINING == False, an existing model will be \n",
    "# loaded and tested against the dataset\n",
    "IS_TRAINING = True\n",
    "\n",
    "# trainingset to be trained or loaded\n",
    "trainingset_number = 5\n",
    "\n",
    "# training params\n",
    "neuron_number = 8\n",
    "activation_function = 'tanh'\n",
    "iterations = 20000\n",
    "learning_rate= 1e-4\n",
    "\n",
    "checkpoint_path = \"./training_\" + str(trainingset_number) + \"/cp_training_weights_only.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "os.mkdir(\"./training_\" + str(trainingset_number))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename_raw_data_real = \"raw_data_real_\" + str(trainingset_number) + \".dat\"\n",
    "# filename_raw_data_imag = \"raw_data_imaginary_\" + str(trainingset_number) + \".dat\"\n",
    "\n",
    "filename_dataset_csv = \"training_data_\" + str(trainingset_number) + \".csv\"\n",
    "\n",
    "\n",
    "dataset_array_numpy = np.loadtxt(filename_dataset_csv,\n",
    "  dtype=float, \n",
    "  delimiter=',',\n",
    "  skiprows=1) # ignore header line\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.experimental.CsvDataset(\n",
    "  filename_dataset_csv,\n",
    "  [tf.float32,  # Required field, use dtype or empty tensor\n",
    "   tf.float32,\n",
    "   tf.float32,\n",
    "   tf.float32\n",
    "  ],\n",
    "  header=True,\n",
    "  select_cols=[0,1,2,3]  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_dataset_structure():\n",
    "\n",
    "  print(dataset)\n",
    "  i = 0\n",
    "\n",
    "  for element in dataset.as_numpy_iterator():\n",
    "    print(element)\n",
    "    i = i + 1\n",
    "    if i > 10:\n",
    "        break\n",
    "\n",
    "show_dataset_structure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform dataset\n",
    "def transform_func(element1, element2, element3, element4):\n",
    "    result = ([element1, element2], [element3, element4])\n",
    "    #result = (element1, element3)\n",
    "    return result\n",
    "\n",
    "# element now has format\n",
    "# (input_re input_imag), (output_re output_imag)\n",
    "\n",
    "dataset = dataset.map(transform_func)\n",
    "\n",
    "show_dataset_structure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IS_TRAINING:\n",
    "    FC_layer = tf.keras.layers.Dense(units=neuron_number, activation=activation_function) # dense layer == fully connected layer    \n",
    "\n",
    "    #input_layer = tf.keras.layers.InputLayer()\n",
    "\n",
    "    #reshape_layer = tf.keras.layers.Reshape()\n",
    "\n",
    "    input_layer = tf.keras.layers.Dense(units=2, input_dim=2)\n",
    "    output_layer = tf.keras.layers.Dense(units=2, activation=activation_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model\n",
    "if IS_TRAINING:\n",
    "    model = tf.keras.Sequential([input_layer, FC_layer, output_layer])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function\n",
    "# as described in article: MSE mean squared error\n",
    "# batches have the format:\n",
    "# [batch_real, batch_imag]\n",
    "def my_loss_fcn(ground_truth_batch, predicted_batch):\n",
    "    print(\"ground_truth_batch:\")\n",
    "    print(ground_truth_batch)\n",
    "    print(\"predicted_batch:\")\n",
    "    print(predicted_batch)\n",
    "    result = tf.square(predicted_batch[:,0] - ground_truth_batch[:,0])\n",
    "    result = result + tf.square(predicted_batch[:,1] - ground_truth_batch[:,1])\n",
    "    N = len(ground_truth_batch)\n",
    "    print(\"N:\"+str(N))\n",
    "    print(result)\n",
    "    result = tf.reduce_sum(result,1) / N\n",
    "    return result  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully compiled new model.\n"
     ]
    }
   ],
   "source": [
    "# compile the keras model\n",
    "# loss=tf.keras.losses.MeanSquaredError()\n",
    "if IS_TRAINING:\n",
    "    model.compile(loss='mse', \n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate), \n",
    "    metrics=['accuracy',\n",
    "    tf.keras.metrics.RootMeanSquaredError()])    \n",
    "\n",
    "    print(\"Successfully compiled new model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model from file if not training\n",
    "if not IS_TRAINING:\n",
    "    model = tf.keras.models.load_model(\"./training_\" + str(trainingset_number) + \"/saved_trained_model\")\n",
    "    print(\"Successfully loaded existing model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_10 (Dense)            (None, 2)                 6         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 8)                 24        \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 2)                 18        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 48\n",
      "Trainable params: 48\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Display the model's architecture\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a callback that saves the model's weights\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=1)\n",
    "\n",
    "# This may generate warnings related to saving the state of the optimizer.\n",
    "# These warnings (and similar warnings throughout this notebook)\n",
    "# are in place to discourage outdated usage, and can be ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20000\n",
      "206460/206471 [============================>.] - ETA: 0s - loss: 0.0010 - accuracy: 0.6519 - root_mean_squared_error: 0.0321\n",
      "Epoch 1: saving model to ./training_5\\cp_test.ckpt\n",
      "206471/206471 [==============================] - 575s 3ms/step - loss: 0.0010 - accuracy: 0.6519 - root_mean_squared_error: 0.0321\n",
      "Epoch 2/20000\n",
      "206461/206471 [============================>.] - ETA: 0s - loss: 0.0010 - accuracy: 0.6519 - root_mean_squared_error: 0.0321\n",
      "Epoch 2: saving model to ./training_5\\cp_test.ckpt\n",
      "206471/206471 [==============================] - 577s 3ms/step - loss: 0.0010 - accuracy: 0.6519 - root_mean_squared_error: 0.0321\n",
      "Epoch 3/20000\n",
      "206460/206471 [============================>.] - ETA: 0s - loss: 0.0010 - accuracy: 0.6519 - root_mean_squared_error: 0.0321\n",
      "Epoch 3: saving model to ./training_5\\cp_test.ckpt\n",
      "206471/206471 [==============================] - 634s 3ms/step - loss: 0.0010 - accuracy: 0.6519 - root_mean_squared_error: 0.0321\n",
      "Epoch 4/20000\n",
      "142864/206471 [===================>..........] - ETA: 2:58 - loss: 0.0010 - accuracy: 0.6519 - root_mean_squared_error: 0.0321"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\jonas\\OneDrive\\Dokumente\\Airbus_Helicopters\\DHBW\\TEN19\\Studienarbeit\\source_code_omar_project\\nn_model.ipynb Cell 15'\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jonas/OneDrive/Dokumente/Airbus_Helicopters/DHBW/TEN19/Studienarbeit/source_code_omar_project/nn_model.ipynb#ch0000013?line=0'>1</a>\u001b[0m \u001b[39m# fit the keras model on the dataset\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jonas/OneDrive/Dokumente/Airbus_Helicopters/DHBW/TEN19/Studienarbeit/source_code_omar_project/nn_model.ipynb#ch0000013?line=1'>2</a>\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jonas/OneDrive/Dokumente/Airbus_Helicopters/DHBW/TEN19/Studienarbeit/source_code_omar_project/nn_model.ipynb#ch0000013?line=2'>3</a>\u001b[0m \u001b[39m# train the model by slicing the data into \"batches\" of size batch_size,\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jonas/OneDrive/Dokumente/Airbus_Helicopters/DHBW/TEN19/Studienarbeit/source_code_omar_project/nn_model.ipynb#ch0000013?line=3'>4</a>\u001b[0m \u001b[39m# and repeatedly iterating over the entire dataset \u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jonas/OneDrive/Dokumente/Airbus_Helicopters/DHBW/TEN19/Studienarbeit/source_code_omar_project/nn_model.ipynb#ch0000013?line=4'>5</a>\u001b[0m \u001b[39m# for a given number of epochs\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jonas/OneDrive/Dokumente/Airbus_Helicopters/DHBW/TEN19/Studienarbeit/source_code_omar_project/nn_model.ipynb#ch0000013?line=5'>6</a>\u001b[0m \u001b[39mif\u001b[39;00m IS_TRAINING:\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/jonas/OneDrive/Dokumente/Airbus_Helicopters/DHBW/TEN19/Studienarbeit/source_code_omar_project/nn_model.ipynb#ch0000013?line=6'>7</a>\u001b[0m     history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jonas/OneDrive/Dokumente/Airbus_Helicopters/DHBW/TEN19/Studienarbeit/source_code_omar_project/nn_model.ipynb#ch0000013?line=7'>8</a>\u001b[0m         dataset_array_numpy[:,\u001b[39m0\u001b[39;49m:\u001b[39m2\u001b[39;49m], \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jonas/OneDrive/Dokumente/Airbus_Helicopters/DHBW/TEN19/Studienarbeit/source_code_omar_project/nn_model.ipynb#ch0000013?line=8'>9</a>\u001b[0m         dataset_array_numpy[:,\u001b[39m2\u001b[39;49m:\u001b[39m4\u001b[39;49m],\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jonas/OneDrive/Dokumente/Airbus_Helicopters/DHBW/TEN19/Studienarbeit/source_code_omar_project/nn_model.ipynb#ch0000013?line=9'>10</a>\u001b[0m         callbacks\u001b[39m=\u001b[39;49m[cp_callback],\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jonas/OneDrive/Dokumente/Airbus_Helicopters/DHBW/TEN19/Studienarbeit/source_code_omar_project/nn_model.ipynb#ch0000013?line=10'>11</a>\u001b[0m         epochs\u001b[39m=\u001b[39;49miterations)\n",
      "File \u001b[1;32mc:\\Users\\jonas\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/jonas/AppData/Local/Programs/Python/Python310/lib/site-packages/keras/utils/traceback_utils.py?line=61'>62</a>\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     <a href='file:///c%3A/Users/jonas/AppData/Local/Programs/Python/Python310/lib/site-packages/keras/utils/traceback_utils.py?line=62'>63</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> <a href='file:///c%3A/Users/jonas/AppData/Local/Programs/Python/Python310/lib/site-packages/keras/utils/traceback_utils.py?line=63'>64</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     <a href='file:///c%3A/Users/jonas/AppData/Local/Programs/Python/Python310/lib/site-packages/keras/utils/traceback_utils.py?line=64'>65</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     <a href='file:///c%3A/Users/jonas/AppData/Local/Programs/Python/Python310/lib/site-packages/keras/utils/traceback_utils.py?line=65'>66</a>\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\jonas\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py:1388\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/jonas/AppData/Local/Programs/Python/Python310/lib/site-packages/keras/engine/training.py?line=1385'>1386</a>\u001b[0m   context\u001b[39m.\u001b[39masync_wait()\n\u001b[0;32m   <a href='file:///c%3A/Users/jonas/AppData/Local/Programs/Python/Python310/lib/site-packages/keras/engine/training.py?line=1386'>1387</a>\u001b[0m logs \u001b[39m=\u001b[39m tmp_logs  \u001b[39m# No error, now safe to assign to logs.\u001b[39;00m\n\u001b[1;32m-> <a href='file:///c%3A/Users/jonas/AppData/Local/Programs/Python/Python310/lib/site-packages/keras/engine/training.py?line=1387'>1388</a>\u001b[0m end_step \u001b[39m=\u001b[39m step \u001b[39m+\u001b[39m data_handler\u001b[39m.\u001b[39;49mstep_increment\n\u001b[0;32m   <a href='file:///c%3A/Users/jonas/AppData/Local/Programs/Python/Python310/lib/site-packages/keras/engine/training.py?line=1388'>1389</a>\u001b[0m callbacks\u001b[39m.\u001b[39mon_train_batch_end(end_step, logs)\n\u001b[0;32m   <a href='file:///c%3A/Users/jonas/AppData/Local/Programs/Python/Python310/lib/site-packages/keras/engine/training.py?line=1389'>1390</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstop_training:\n",
      "File \u001b[1;32mc:\\Users\\jonas\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\data_adapter.py:1266\u001b[0m, in \u001b[0;36mDataHandler.step_increment\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/jonas/AppData/Local/Programs/Python/Python310/lib/site-packages/keras/engine/data_adapter.py?line=1262'>1263</a>\u001b[0m       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_current_step \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m steps_remaining\n\u001b[0;32m   <a href='file:///c%3A/Users/jonas/AppData/Local/Programs/Python/Python310/lib/site-packages/keras/engine/data_adapter.py?line=1263'>1264</a>\u001b[0m       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_steps_per_execution\u001b[39m.\u001b[39massign(original_spe)\n\u001b[1;32m-> <a href='file:///c%3A/Users/jonas/AppData/Local/Programs/Python/Python310/lib/site-packages/keras/engine/data_adapter.py?line=1265'>1266</a>\u001b[0m \u001b[39m@property\u001b[39m\n\u001b[0;32m   <a href='file:///c%3A/Users/jonas/AppData/Local/Programs/Python/Python310/lib/site-packages/keras/engine/data_adapter.py?line=1266'>1267</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep_increment\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m   <a href='file:///c%3A/Users/jonas/AppData/Local/Programs/Python/Python310/lib/site-packages/keras/engine/data_adapter.py?line=1267'>1268</a>\u001b[0m   \u001b[39m\"\"\"The number to increment the step for `on_batch_end` methods.\"\"\"\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/jonas/AppData/Local/Programs/Python/Python310/lib/site-packages/keras/engine/data_adapter.py?line=1268'>1269</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_step_increment\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# fit the keras model on the dataset\n",
    "#\n",
    "# train the model by slicing the data into \"batches\" of size batch_size,\n",
    "# and repeatedly iterating over the entire dataset \n",
    "# for a given number of epochs\n",
    "if IS_TRAINING:\n",
    "    history = model.fit(\n",
    "        dataset_array_numpy[:,0:2], \n",
    "        dataset_array_numpy[:,2:4],\n",
    "        callbacks=[cp_callback],\n",
    "        epochs=iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the dataset against the loaded model\n",
    "if not IS_TRAINING:\n",
    "    model.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./training_5/saved_trained_model\\assets\n"
     ]
    }
   ],
   "source": [
    "# save the model in the correct directory\n",
    "if IS_TRAINING:\n",
    "    model.save(\"./training_\" + str(trainingset_number) + \"/saved_trained_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The returned history object holds a record of the loss values \n",
    "# and metric values during training\n",
    "print(history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training progress\n",
    "# diagram number 2 paper page 4\n",
    "\n",
    "# cost function mean square error"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "683e9bbf599fde3b00e37a0db68ad40a268db525b46af3924c3427b16ddb8792"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
